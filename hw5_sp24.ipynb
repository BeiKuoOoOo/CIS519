{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeiKuoOoOo/CIS519/blob/main/hw5_sp24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0yamHN-dY0"
      },
      "source": [
        "# **CIS 4190/5190 Spring 2024 - Homework 5**\n",
        "\n",
        "**Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. This is the master notebook so <u>you will not be able to save your changes without copying it </u>! Once you click on that, make sure you are working on that version of the notebook so that your work is saved**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX6m4rCai_mC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451d8c83-114e-4d8e-bf76-ba618018956b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/116.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m112.6/116.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.8\n"
          ]
        }
      ],
      "source": [
        "# Restart the runtime after running this cell everytime you open the notebook\n",
        "!pip install dill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb-WLp5Z-cdy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "from sklearn import preprocessing\n",
        "np.random.seed(42)  # don't change this line\n",
        "\n",
        "import base64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjjXBdEb-p8K"
      },
      "source": [
        "# **PennGrader Setup**\n",
        "\n",
        "First, you'll need to set up the PennGrader, an autograder we are going to use throughout the semester. The PennGrader will automatically grade your answer and provide you with an instant feedback. Unless otherwise stated, you can resubmit up to a reasonable number of attempts (e.g. 100 attemptes per day). **We will only record your latest score in our backend database**.\n",
        "\n",
        "After finishing each homework assignment, you must submit your iPython notebook to gradescope before the homework deadline. Gradescope will then retrive and display your scores from our backend database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GCTLN4G-nK2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install penngrader --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLnoPRci-sTC"
      },
      "outputs": [],
      "source": [
        "from penngrader.grader import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu0XYZHO-t8J"
      },
      "outputs": [],
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID =           # YOUR PENN-ID GOES HERE AS AN INTEGER#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIDTGGbo-xkf"
      },
      "source": [
        "Run the following cell to initialize the autograder. This autograder will let you submit your code directly from this notebook and immidiately get a score.\n",
        "\n",
        "**NOTE:** Remember we store your submissions and check against other student's submissions... so, not that you would, but no cheating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw_QDnZk-vvI"
      },
      "outputs": [],
      "source": [
        "grader = PennGrader(homework_id = 'cis5190_sp24_HW5', student_id = STUDENT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQCyLhELJ7LX"
      },
      "source": [
        "#### **NOTE 1. Results of sections marked as \"manually graded\" should be submitted along with the written homework solutions.**\n",
        "\n",
        "#### **NOTE 2. If you are running into a `__builtins__' error, it's likely because you're using a function call of the form numpy.ndarray.mean(), like a.mean(). This does not play nice with PennGrader unfortunately. Please use the function call numpy.mean(a) instead.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoidSevnpiqu"
      },
      "source": [
        "# **1. [20 pts] Image Classification using CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaLhD7tcacNK"
      },
      "source": [
        "#### **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgTR67PEQhn6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydDc504-akhk"
      },
      "source": [
        "#### **Set the random seed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgoqGPBPdGBf"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2bidiHXawdT"
      },
      "source": [
        "#### **Set GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lI0V_BYa3LP"
      },
      "outputs": [],
      "source": [
        "# Make sure you're using cuda (GPU) by checking the hardware accelerator under Runtime -> Change runtime type\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"We're using:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dH1H9cSS4nc"
      },
      "source": [
        "#### **Download and extract the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGXwhaFyjr-J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ry7qltBzb_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6de56c-e11f-4522-d9c3-ba0705c7746d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vacRphjb47LXifcd3F2xlsOTKR0R_LiF\n",
            "From (redirected): https://drive.google.com/uc?id=1vacRphjb47LXifcd3F2xlsOTKR0R_LiF&confirm=t&uuid=37e26b91-47c5-405b-9db0-d71fe6282c0d\n",
            "To: /content/supertuxkart_data.zip\n",
            "100% 46.2M/46.2M [00:00<00:00, 139MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1vacRphjb47LXifcd3F2xlsOTKR0R_LiF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAhGG46XzibJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!yes N | unzip \"/content/supertuxkart_data.zip\" -d \"/content\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWRZZ6Gd3O5f"
      },
      "source": [
        "## **1.1. Dataset class implementation**\n",
        "\n",
        "In this section, you will be training, validating and testing a CNN model to classify images of objects from a car racing video game called SuperTuxKart. There are 6 classes of objects: kart is 1, pickup is 2, nitro is 3, bomb is 4 and projectile 5. The background class (all other images) is assigned the label 0. First, you need to load data in a way that PyTorch can deal with easily. We will lean on PyTorch’s `Dataset` class to do this.\n",
        "\n",
        "Complete the `STKDataset` class that inherits from `Dataset`.\n",
        "\n",
        "1. `__init__` is a constructor, and would be the natural place to perform operations common to the full dataset, such as parsing the labels and image paths.\n",
        "2. The `__len__` function should return the size of the dataset, i.e., the number of samples.\n",
        "3. The `__getitem__` function should return a python tuple of (image, label). The image should be a torch.Tensor of size (3, 64, 64) and the label should be an int.\n",
        "\n",
        "The labels of the images under a particular folder (`train/` or `val/`) are stored in the same folder as `labels.csv`. Read the `labels.csv` file using `pandas` to understand what it looks like before proceeding. There is also a `labels.csv` in the `test/` folder. That would only contain the file names of the test samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clNM_8vS4r_0"
      },
      "outputs": [],
      "source": [
        "ENCODING_TO_LABELS = {0: \"background\",\n",
        "                    1: \"kart\",\n",
        "                    2: \"pickup\",\n",
        "                    3: \"nitro\",\n",
        "                    4: \"bomb\",\n",
        "                    5: \"projectile\"}\n",
        "\n",
        "LABELS_TO_ENCODING = {\"background\": 0,\n",
        "                    \"kart\": 1,\n",
        "                    \"pickup\": 2,\n",
        "                    \"nitro\": 3,\n",
        "                    \"bomb\": 4,\n",
        "                    \"projectile\": 5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMx57cqWySrk"
      },
      "outputs": [],
      "source": [
        "class STKDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_path, transform=None):\n",
        "        self.image_path = image_path\n",
        "        self.labels = pd.read_csv(image_path + \"/labels.csv\")\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # STUDENT TODO START: Return the number of samples in the dataset\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # STUDENT TODO START: Create the path to each image by joining the root path with the name of the file as found in labels.csv\n",
        "        # STUDENT TODO END\n",
        "\n",
        "        # Read the image from the file path\n",
        "        image = Image.open(img_name)\n",
        "        # Transform the image using self.transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if \"label\" in self.labels.columns:\n",
        "            # STUDENT TODO START: Extract label name and encode it using the LABELS_TO_ENCODING dictionary\n",
        "\n",
        "            # STUDENT TODO END\n",
        "            sample = (image, label)\n",
        "        else:\n",
        "            sample = (image)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVLSRk2r4wff"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START: Use transforms.Compose to transform the image such that every pixel takes on a value between -1 and 1\n",
        "# Hint: Refer to transforms.ToTensor() and transforms.Normalize()\n",
        "\n",
        "# STUDENT TODO END\n",
        "\n",
        "train_dataset = STKDataset(image_path=\"train\", transform=transform)\n",
        "val_dataset = STKDataset(image_path=\"val\", transform=transform)\n",
        "test_dataset = STKDataset(image_path=\"test\", transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xU1vMHZTZqA"
      },
      "source": [
        "#### **Visualization**\n",
        "\n",
        "The following cell visualizes the data as a sanity check for your implementation of the `STKDataset` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qneu5Tqu5HwP"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "torch.manual_seed(0)\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
        "    img, label = train_dataset[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(ENCODING_TO_LABELS[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.permute(1, 2, 0)*0.5 + 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO62UxlvT_-B"
      },
      "source": [
        "#### **Data loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEeDpfBn-FjK"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START: Create data loaders for training, validation, and test sets each having a batch size of 64.\n",
        "# Set shuffle to be True for training and validation data loaders, False for test data loader.\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc_f7OeMcaeh"
      },
      "source": [
        "## **1.2. CNN architecture**\n",
        "\n",
        "Your goal is to devise a CNN that passes the threshold accuracy (80%) on the test set. You get full score (20 pts) if you get at least 80% test set accuracy and 0 if you get 30% or below. The score varies linearly between 0 and 20 for accuracies between 30% and 80%.\n",
        "\n",
        "There are several decisions that you take in building your CNN including but not limited to:\n",
        "\n",
        "- the number of convolutional layers\n",
        "- the kernel size, stride, padding and number of out channels for each convolutional layer\n",
        "- number of fully connected layers\n",
        "- number of nodes in each fully connected layer\n",
        "\n",
        "You are free to decide the architecture. To make your search easier, we recommend you to use not more than four convolutional layers and four fully connected layers. We also suggest that you use the relu activation function between the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CJJEh5rBRmQ"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # STUDENT TODO START: Create the layers of your CNN here\n",
        "\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def forward(self, x):\n",
        "        # STUDENT TODO START: Perform the forward pass through the layers\n",
        "\n",
        "        # STUDENT TODO END\n",
        "\n",
        "# STUDENT TODO START: Create an instance of Net and move it to the GPU\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo7XH85FfrXl"
      },
      "source": [
        "## **1.2. Training, validation, and testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hfsnLHCCxt5"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START:\n",
        "# 1. Set the criterion to be cross entropy loss\n",
        "\n",
        "\n",
        "# 2. Experiment with different optimizers\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dECXr5bCz-6"
      },
      "outputs": [],
      "source": [
        "train_loss, validation_loss = [], []\n",
        "train_acc, validation_acc = [], []\n",
        "\n",
        "# STUDENT TODO START:\n",
        "# Note that we have set the number of epochs to be 10. You can choose to increase or decrease the number of epochs.\n",
        "\n",
        "        # 1. Store the inputs and labels in the GPU\n",
        "\n",
        "\n",
        "        # 2. Get the model predictions\n",
        "\n",
        "\n",
        "        # 3. Zero the gradients out\n",
        "\n",
        "\n",
        "        # 4. Get the loss\n",
        "\n",
        "\n",
        "        # 5. Calculate the gradients\n",
        "\n",
        "\n",
        "        # 6. Update the weights\n",
        "\n",
        "\n",
        "    # Validation\n",
        "\n",
        "        # 1. Store the inputs and labels in the GPU\n",
        "\n",
        "\n",
        "        # 2. Get the model predictions\n",
        "\n",
        "\n",
        "        # 3. Get the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be97yhnt4sAf"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "test_predictions = np.array([])\n",
        "\n",
        "for i, data in enumerate(test_dataloader, 0):\n",
        "\n",
        "    inputs = data\n",
        "    # STUDENT TODO START:\n",
        "    # 1. Store the inputs in the GPU\n",
        "\n",
        "    # 2. Get the model predictions\n",
        "\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    _, predicted = torch.max(predictions, 1)\n",
        "\n",
        "    test_predictions = np.concatenate((test_predictions, predicted.detach().cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSzSgCbYH8Zi"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_cnn_predictions', answer = test_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h9O4ky189kj"
      },
      "source": [
        "Download the .ipynb notebook and submit on Gradescope."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}